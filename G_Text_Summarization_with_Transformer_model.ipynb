{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this [exercise](G_Text_Summarization_with_Transformer_model.ipynb), I implemented the decoder part of a transformer model for text summarization task, based on the starting code, instructions, and utility functions from the [Natural Language Processing with Attention Models](https://www.coursera.org/learn/attention-models-in-nlp) course (by DeepLearning.AI on Coursera).\n",
    "\n",
    "Coursera starting code already handled text pre-processing as well as other parts of this Transformer model. \n",
    "\n",
    "![transformer.png](images%2Ftransformer.png)\n",
    "\n",
    "This exercise is mainly for practice, so the text summarization result was not very accurate. The model can generate summaries that have at least some resemblance to the original paragraphs in the training data, but could not do the same with testing paragraphs. \n",
    "\n",
    "That shows that the transformer model was overfitting, given how big the transformer model was and how small Coursera's training data was. The model might be memorizing the summary sentences provided in the training data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aff89d8ae2eb7fed"
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0Ryl3wZOsTt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636168103,
     "user_tz": -420,
     "elapsed": 31026,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "4a2c6e5e-212b-4386-e75d-4f110d0fc1eb"
   },
   "id": "m0Ryl3wZOsTt",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# %cd /content/drive/Othercomputers/My Laptop/My_NLP_notebooks/C4W2 Assignment/Files/tf\n",
    "# %ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWoSJogROtCM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636172977,
     "user_tz": -420,
     "elapsed": 2964,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "32846112-dde9-4fc7-e87c-750404c31768"
   },
   "id": "aWoSJogROtCM",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/Othercomputers/My Laptop/My_NLP_notebooks/C4W2 Assignment/Files/tf\n",
      "C4W2_Assignment.ipynb                              \u001B[0m\u001B[01;34mPythonScript\u001B[0m/\n",
      "\u001B[01;34mdata\u001B[0m/                                              readme.md\n",
      "G_Text_Summarization_with_Transformer_model.ipynb  T_C4W2_Assignment.ipynb\n",
      "\u001B[01;34mimages\u001B[0m/                                            T_C4W2_Assignment_passed.ipynb\n",
      "notebook2script.py                                 utils.py\n",
      "\u001B[01;34m__pycache__\u001B[0m/                                       w2_unittest.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# %pip install tensorflow-text"
   ],
   "metadata": {
    "id": "zXMuJ7frO61v"
   },
   "id": "zXMuJ7frO61v",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-07T13:13:07.533820800Z",
     "start_time": "2024-01-07T13:12:46.587198400Z"
    },
    "id": "initial_id",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636176896,
     "user_tz": -420,
     "elapsed": 3923,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing the Data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fb61965c01c51142"
   },
   "id": "fb61965c01c51142"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dialogue:\n",
      "Lucas: Hey! How was your day?\r\n",
      "Demi: Hey there! \r\n",
      "Demi: It was pretty fine, actually, thank you!\r\n",
      "Demi: I just got promoted! :D\r\n",
      "Lucas: Whoa! Great news!\r\n",
      "Lucas: Congratulations!\r\n",
      "Lucas: Such a success has to be celebrated.\r\n",
      "Demi: I agree! :D\r\n",
      "Demi: Tonight at Death & Co.?\r\n",
      "Lucas: Sure!\r\n",
      "Lucas: See you there at 10pm?\r\n",
      "Demi: Yeah! See you there! :D\n",
      "\n",
      "Summary:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/corpus\"\n",
    "\n",
    "train_data, test_data = utils.get_train_test_data(data_dir)\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:13:08.292406300Z",
     "start_time": "2024-01-07T13:13:07.549287400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a1a162a67f83a05",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636179882,
     "user_tz": -420,
     "elapsed": 2997,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "fd150560-0f83-4020-9d45-bffca458bbe6"
   },
   "id": "8a1a162a67f83a05"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# add the [EOS] token to the end of sentences and [SOS] to the beginning\n",
    "document, summary = utils.preprocess(train_data)\n",
    "document_test, summary_test = utils.preprocess(test_data)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:13:22.077503900Z",
     "start_time": "2024-01-07T13:13:17.173361200Z"
    },
    "id": "c82063e8a3fe70f3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636180667,
     "user_tz": -420,
     "elapsed": 790,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "c82063e8a3fe70f3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of vocabulary: 34250\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary by combining the documents and the summaries and use .fit_on_texts() method\n",
    "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
    "oov_token = '[UNK]'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
    "\n",
    "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
    "\n",
    "tokenizer.fit_on_texts(documents_and_summary)\n",
    "\n",
    "inputs = tokenizer.texts_to_sequences(document)\n",
    "targets = tokenizer.texts_to_sequences(summary)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'Size of vocabulary: {vocab_size}')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:14:22.218347200Z",
     "start_time": "2024-01-07T13:14:05.327977900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7cbc70ba06c856e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636182786,
     "user_tz": -420,
     "elapsed": 2125,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "d34e6b78-bf52-4059-83e5-2acb70a4c10d"
   },
   "id": "b7cbc70ba06c856e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# pad the tokenized sequences\n",
    "# Limit the size of the input and output data\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "# Pad the sequences.\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:14:38.830131700Z",
     "start_time": "2024-01-07T13:14:36.596249800Z"
    },
    "id": "6423a222d38295ed",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636188217,
     "user_tz": -420,
     "elapsed": 2655,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "6423a222d38295ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "id": "23437d13f057e608"
   },
   "id": "23437d13f057e608"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings\n",
    "\n",
    "    Arguments:\n",
    "        positions (int): Maximum number of positions to be encoded\n",
    "        d_model (int): Encoding size\n",
    "\n",
    "    Returns:\n",
    "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "\n",
    "    # initialize a matrix angle_rads of all the angles\n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:15:38.005264200Z",
     "start_time": "2024-01-07T13:15:37.682063300Z"
    },
    "id": "e9267ffa475893a2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636190376,
     "user_tz": -420,
     "elapsed": 8,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "e9267ffa475893a2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# \"padding mask\" and \"look-ahead mask\" help the softmax computation within the QKV attention layer ignore the tokens that are just padding, or ignore tokens that the model has not seen while trying to produce an output sequence\n",
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "\n",
    "    Arguments:\n",
    "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
    "\n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
    "    \"\"\"\n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    # this will allow for broadcasting later when comparing sequences\n",
    "    return seq[:, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "\n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "\n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:17:06.158764300Z",
     "start_time": "2024-01-07T13:17:05.925898200Z"
    },
    "id": "c6262a44921d1f56",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636194796,
     "user_tz": -420,
     "elapsed": 727,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "c6262a44921d1f56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-Attention"
   ],
   "metadata": {
    "collapsed": false,
    "id": "954976aaacb054a"
   },
   "id": "954976aaacb054a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead)\n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
    "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
    "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
    "        mask (tf.Tensor): mask with shape broadcastable\n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    # Multiply q and k transposed.\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk with the square root of dk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:  # Don't replace this None\n",
    "        scaled_attention_logits += (1 - mask) * -1e9\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.keras.activations.softmax(scaled_attention_logits)\n",
    "\n",
    "    # Multiply the attention weights by v\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:17:36.493913500Z",
     "start_time": "2024-01-07T13:17:36.292948500Z"
    },
    "id": "c2ac6d8dfd89af37",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636198205,
     "user_tz": -420,
     "elapsed": 526,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "c2ac6d8dfd89af37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b20bff3a04ac6818"
   },
   "id": "b20bff3a04ac6818"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"400\"/>\n",
    "<caption><center><font color='purple'><b>Transformer encoder layer</font></center></caption>\n",
    "Combining multi-head attention and feed forward neural network together into an encoder layer. The encoder block is this layer repeated several times.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "145b324664e56e9c"
   },
   "id": "145b324664e56e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    \"\"\"\n",
    "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
    "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
    "    embedding_dim and no activation.\n",
    "\n",
    "    Arguments:\n",
    "        embedding_dim (int): output dimension\n",
    "        fully_connected_dim (int): dimension of the hidden layer\n",
    "\n",
    "    Returns:\n",
    "        _ (tf.keras.Model): sequential model\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:21:41.371235Z",
     "start_time": "2024-01-07T13:21:41.093547Z"
    },
    "id": "44028bd44169be52",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636202951,
     "user_tz": -420,
     "elapsed": 7,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "44028bd44169be52"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network.\n",
    "    This architecture includes a residual connection around each of the two\n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha(~1 line).\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # skip connection\n",
    "        # apply layer normalization on sum of the input and the attention output to get the\n",
    "        # output of the multi-head attention layer\n",
    "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # apply dropout layer to ffn output during training\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "\n",
    "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
    "        # to get the output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        return encoder_layer_out\n",
    ""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:22:52.090983500Z",
     "start_time": "2024-01-07T13:22:51.885087300Z"
    },
    "id": "9bc470f93137c6bf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636204428,
     "user_tz": -420,
     "elapsed": 3,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "9bc470f93137c6bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "\n",
    "full encoder block (embedded inputs and positional encoding are also added as input)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6216fd3bd41ceb10"
   },
   "id": "6216fd3bd41ceb10"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# full encoder block (embedded inputs and positional encoding are also added as input)\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer\n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps)\n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:24:44.561366600Z",
     "start_time": "2024-01-07T13:24:44.345458Z"
    },
    "id": "25132996e34fbcb3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636207088,
     "user_tz": -420,
     "elapsed": 4,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "25132996e34fbcb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eb94fd1bbc7a1048"
   },
   "id": "eb94fd1bbc7a1048"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/decoder_layer.png\" alt=\"Decoder\" width=\"250\"/>\n",
    "\n",
    "The decoder layer takes the K and V matrices that are the output of the encoder, and pass into the second multi-head attention layer which will also take in the Q matrix that comes from the first multi-head attention layer."
   ],
   "metadata": {
    "collapsed": false,
    "id": "baebd95e23fdcfd5"
   },
   "id": "baebd95e23fdcfd5"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks,\n",
    "    one that takes the new input and uses self-attention, and the other\n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
    "        # Dropout will be applied during training (~1 line).\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, return_attention_scores=True, training=training)\n",
    "\n",
    "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
    "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
    "        # Dropout will be applied during training\n",
    "        # Return attention scores as attn_weights_block2 (~1 line)\n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(query=Q1, value=enc_output, key=enc_output, attention_mask=padding_mask, return_attention_scores=True, training=training)\n",
    "\n",
    "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block (~1 line)\n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)\n",
    "\n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "\n",
    "        # apply a dropout layer to the ffn output\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "\n",
    "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
    "        out3 = self.layernorm3(ffn_output + mult_attn_out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    ""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:28:18.901168300Z",
     "start_time": "2024-01-07T13:28:18.617299400Z"
    },
    "id": "57f1deb77769d916",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636212225,
     "user_tz": -420,
     "elapsed": 4,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "57f1deb77769d916"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/decoder.png\" alt=\"Decoder\" width=\"300\"/>\n",
    "\n",
    "Full decoder: the decoder layer is repeated multiple times. Positional encoding will also be added as the input to the block."
   ],
   "metadata": {
    "collapsed": false,
    "id": "384ca964b3e5adb2"
   },
   "id": "384ca964b3e5adb2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer\n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps)\n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # create word embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # add positional encodings to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # apply a dropout layer to x\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "\n",
    "            x, block1, block2 = self.dec_layers[i](x=x, enc_output=enc_output,\n",
    "                                                   training=training,\n",
    "                                                   look_ahead_mask=look_ahead_mask,\n",
    "                                                   padding_mask=padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:28:10.032930300Z",
     "start_time": "2024-01-07T13:28:09.793430900Z"
    },
    "id": "9193ef99668a1271",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636213464,
     "user_tz": -420,
     "elapsed": 7,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "9193ef99668a1271"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d99bcd960e2db05b"
   },
   "id": "d99bcd960e2db05b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "\n",
    "Transformer model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ca7c4b077af0cfa1"
   },
   "id": "ca7c4b077af0cfa1"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output (tf.Tensor): The final output of the model\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "        \"\"\"\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(x=input_sentence, training=training, mask=enc_padding_mask)\n",
    "\n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(x=output_sentence,\n",
    "                                                     enc_output=enc_output,\n",
    "                                                     training=training,\n",
    "                                                     look_ahead_mask=look_ahead_mask,\n",
    "                                                     padding_mask=dec_padding_mask)\n",
    "\n",
    "        # pass decoder output through a linear layer and softmax (~1 line)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:29:58.188753200Z",
     "start_time": "2024-01-07T13:29:57.866880700Z"
    },
    "id": "47c500a4eb0afb41",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636218612,
     "user_tz": -420,
     "elapsed": 518,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "47c500a4eb0afb41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b36141353868c242"
   },
   "id": "b36141353868c242"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    num_layers,\n",
    "    embedding_dim,\n",
    "    num_heads,\n",
    "    fully_connected_dim,\n",
    "    vocab_size,\n",
    "    vocab_size,\n",
    "    positional_encoding_length,\n",
    "    positional_encoding_length,\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:30:46.599392400Z",
     "start_time": "2024-01-07T13:30:46.150270200Z"
    },
    "id": "ae693719fa37338d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636219614,
     "user_tz": -420,
     "elapsed": 8,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "ae693719fa37338d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(embedding_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:30:46.915493200Z",
     "start_time": "2024-01-07T13:30:46.584400700Z"
    },
    "id": "125126b965e05c92",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636220769,
     "user_tz": -420,
     "elapsed": 6,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "125126b965e05c92"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# need to apply a mask when calculating the loss because the sequences have been padded\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "def masked_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Here you will store the losses, so you can later plot them\n",
    "losses = []"
   ],
   "metadata": {
    "id": "720d8888a4208d1c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636223638,
     "user_tz": -420,
     "elapsed": 6,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "720d8888a4208d1c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, inp, tar):\n",
    "    \"\"\"\n",
    "    One training step for the transformer\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input data to summarize\n",
    "        tar (tf.Tensor): Target (summary)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "    dec_padding_mask = create_padding_mask(inp) # Notice that both encoder and decoder padding masks are equal\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = model(\n",
    "            inp,\n",
    "            tar_inp,\n",
    "            True,\n",
    "            enc_padding_mask,\n",
    "            look_ahead_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = masked_loss(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ],
   "metadata": {
    "id": "7bbd01b416f40c6d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636225271,
     "user_tz": -420,
     "elapsed": 4,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "7bbd01b416f40c6d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "def next_word(model, encoder_input, output):\n",
    "    \"\"\"\n",
    "    Helper function for summarization that uses the model to predict just the next word.\n",
    "    Arguments:\n",
    "        encoder_input (tf.Tensor): Input data to summarize\n",
    "        output (tf.Tensor): (incomplete) target (summary)\n",
    "    Returns:\n",
    "        predicted_id (tf.Tensor): The id of the predicted word\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Create a padding mask for the input (encoder)\n",
    "    enc_padding_mask = create_padding_mask(encoder_input)\n",
    "    # Create a look-ahead mask for the output\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
    "    # Create a padding mask for the input (decoder)\n",
    "    dec_padding_mask = create_padding_mask(encoder_input)\n",
    "\n",
    "    # Run the prediction of the next word with the transformer model\n",
    "    predictions, attention_weights = model(\n",
    "        encoder_input,\n",
    "        output,\n",
    "        False,\n",
    "        enc_padding_mask,\n",
    "        look_ahead_mask,\n",
    "        dec_padding_mask,\n",
    "    )\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    predictions = predictions[: ,-1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    return predicted_id"
   ],
   "metadata": {
    "id": "1aa0e365410c92d1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636229890,
     "user_tz": -420,
     "elapsed": 4,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "1aa0e365410c92d1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def summarize(model, input_document):\n",
    "    \"\"\"\n",
    "    A function for summarization using the transformer model\n",
    "    Arguments:\n",
    "        input_document (tf.Tensor): Input data to summarize\n",
    "    Returns:\n",
    "        _ (str): The summary of the input_document\n",
    "    \"\"\"\n",
    "    input_document = tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
    "\n",
    "    for i in range(decoder_maxlen):\n",
    "        predicted_id = next_word(model, encoder_input, output)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
    "            break\n",
    "\n",
    "    return tokenizer.sequences_to_texts(output.numpy())[0]  # since there is just one translated document"
   ],
   "metadata": {
    "id": "6925fde3cc2845f5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704636231001,
     "user_tz": -420,
     "elapsed": 9,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    }
   },
   "id": "6925fde3cc2845f5"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1, Loss 4.2428\n",
      "Time taken for one epoch: 21.562100887298584 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] alice wants to go to the cinema with her [EOS]\n",
      "\n",
      "Epoch 2, Loss 4.1426\n",
      "Time taken for one epoch: 21.398553133010864 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is going to the cinema with alice and alice [EOS]\n",
      "\n",
      "Epoch 3, Loss 4.0420\n",
      "Time taken for one epoch: 21.49729824066162 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah has just arrived and he will see the movie at the cinema with amanda [EOS]\n",
      "\n",
      "Epoch 4, Loss 3.9512\n",
      "Time taken for one epoch: 21.430785655975342 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] alice and hannah are going to the cinema with amanda and sara [EOS]\n",
      "\n",
      "Epoch 5, Loss 3.8530\n",
      "Time taken for one epoch: 21.592316389083862 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah has just arrived to the library with amanda and sara will see him [EOS]\n",
      "\n",
      "Epoch 6, Loss 3.7601\n",
      "Time taken for one epoch: 21.584917783737183 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is going to the store with amanda and sara will go to the cinema [EOS]\n",
      "\n",
      "Epoch 7, Loss 3.6650\n",
      "Time taken for one epoch: 21.516125202178955 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah has just finished the phone he will send hannah the number [EOS]\n",
      "\n",
      "Epoch 8, Loss 3.5785\n",
      "Time taken for one epoch: 21.516773462295532 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is going to the store with amanda and sara will be at the park [EOS]\n",
      "\n",
      "Epoch 9, Loss 3.4850\n",
      "Time taken for one epoch: 21.526968479156494 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is going to the store with amanda and hannah [EOS]\n",
      "\n",
      "Epoch 10, Loss 3.3998\n",
      "Time taken for one epoch: 21.599140167236328 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is angry with amanda and sara as he has to do it [EOS]\n",
      "\n",
      "Epoch 11, Loss 3.3160\n",
      "Time taken for one epoch: 21.500353813171387 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for the phone but he doesn't know what happened to him [EOS]\n",
      "\n",
      "Epoch 12, Loss 3.2333\n",
      "Time taken for one epoch: 21.57628846168518 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for the report for amanda as he doesn't know if he doesn't know if she doesn't know if she can [EOS]\n",
      "\n",
      "Epoch 13, Loss 3.1525\n",
      "Time taken for one epoch: 21.63994860649109 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for an event on the way to see it again [EOS]\n",
      "\n",
      "Epoch 14, Loss 3.0690\n",
      "Time taken for one epoch: 21.52720069885254 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for the report for amanda as he doesn't know what to do [EOS]\n",
      "\n",
      "Epoch 15, Loss 2.9907\n",
      "Time taken for one epoch: 21.524073600769043 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and amanda have to check the report for amanda [EOS]\n",
      "\n",
      "Epoch 16, Loss 2.9128\n",
      "Time taken for one epoch: 21.554113626480103 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know what he doesn't know if he doesn't know what to do [EOS]\n",
      "\n",
      "Epoch 17, Loss 2.8367\n",
      "Time taken for one epoch: 21.525733947753906 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know what she doesn't want to go to the park for himself [EOS]\n",
      "\n",
      "Epoch 18, Loss 2.7641\n",
      "Time taken for one epoch: 21.726847171783447 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she doesn't know what happened to him they will do something [EOS]\n",
      "\n",
      "Epoch 19, Loss 2.6921\n",
      "Time taken for one epoch: 21.634623765945435 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she doesn't know what happened to him [EOS]\n",
      "\n",
      "Epoch 20, Loss 2.6245\n",
      "Time taken for one epoch: 21.654727458953857 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she doesn't know if she doesn't know if she doesn't know if she doesn't know if she doesn't know if she doesn't know if she can [EOS]\n",
      "\n",
      "Epoch 21, Loss 2.5554\n",
      "Time taken for one epoch: 21.563578128814697 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know what she doesn't want to find out for amanda and doesn't know if she doesn't want to do something [EOS]\n",
      "\n",
      "Epoch 22, Loss 2.4835\n",
      "Time taken for one epoch: 21.526515007019043 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset with amanda and amanda who forgot about the park [EOS]\n",
      "\n",
      "Epoch 23, Loss 2.4196\n",
      "Time taken for one epoch: 21.46767807006836 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she can find something else on amanda doesn't want to do something else [EOS]\n",
      "\n",
      "Epoch 24, Loss 2.3569\n",
      "Time taken for one epoch: 21.588441848754883 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has yet because she has to do something [EOS]\n",
      "\n",
      "Epoch 25, Loss 2.2932\n",
      "Time taken for one epoch: 21.4442355632782 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just returned from the park but didn't know anything that she doesn't want to do something [EOS]\n",
      "\n",
      "Epoch 26, Loss 2.2253\n",
      "Time taken for one epoch: 21.628479957580566 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she can find something else on the way [EOS]\n",
      "\n",
      "Epoch 27, Loss 2.1651\n",
      "Time taken for one epoch: 21.692426681518555 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and amanda have any number as they did not know what happened [EOS]\n",
      "\n",
      "Epoch 28, Loss 2.1093\n",
      "Time taken for one epoch: 21.478107452392578 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any number but didn't know anything [EOS]\n",
      "\n",
      "Epoch 29, Loss 2.0534\n",
      "Time taken for one epoch: 21.512264728546143 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has changed her phone and she doesn't know what to do [EOS]\n",
      "\n",
      "Epoch 30, Loss 1.9949\n",
      "Time taken for one epoch: 21.577738046646118 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any news for amanda and she doesn't know what to do [EOS]\n",
      "\n",
      "Epoch 31, Loss 1.9453\n",
      "Time taken for one epoch: 21.59257674217224 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not read it as she has recently [EOS]\n",
      "\n",
      "Epoch 32, Loss 1.8883\n",
      "Time taken for one epoch: 21.66460919380188 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she can find anything about it amanda doesn't know if she doesn't want to do [EOS]\n",
      "\n",
      "Epoch 33, Loss 1.8348\n",
      "Time taken for one epoch: 21.589678049087524 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not read yet [EOS]\n",
      "\n",
      "Epoch 34, Loss 1.7858\n",
      "Time taken for one epoch: 21.61901879310608 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not yet [EOS]\n",
      "\n",
      "Epoch 35, Loss 1.7374\n",
      "Time taken for one epoch: 21.612913608551025 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has changed her number [EOS]\n",
      "\n",
      "Epoch 36, Loss 1.6891\n",
      "Time taken for one epoch: 21.547157049179077 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not read yet [EOS]\n",
      "\n",
      "Epoch 37, Loss 1.6425\n",
      "Time taken for one epoch: 21.532472133636475 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is worried about the way back but she can't believe she doesn't want to do something [EOS]\n",
      "\n",
      "Epoch 38, Loss 1.5983\n",
      "Time taken for one epoch: 21.613391160964966 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset with amanda and the number [EOS]\n",
      "\n",
      "Epoch 39, Loss 1.5491\n",
      "Time taken for one epoch: 21.56514620780945 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just called the number but didn't know anything that she doesn't want to get any [EOS]\n",
      "\n",
      "Epoch 40, Loss 1.5091\n",
      "Time taken for one epoch: 21.648282527923584 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any answer the number of himself [EOS]\n",
      "\n",
      "Epoch 41, Loss 1.4647\n",
      "Time taken for one epoch: 21.635285139083862 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any news for amanda and the meeting [EOS]\n",
      "\n",
      "Epoch 42, Loss 1.4233\n",
      "Time taken for one epoch: 21.625343322753906 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any news that will be easily amused [EOS]\n",
      "\n",
      "Epoch 43, Loss 1.3888\n",
      "Time taken for one epoch: 21.674967288970947 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not solve other and and and she can't solve the conversation [EOS]\n",
      "\n",
      "Epoch 44, Loss 1.3520\n",
      "Time taken for one epoch: 21.606512784957886 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just returned from the report for amanda [EOS]\n",
      "\n",
      "Epoch 45, Loss 1.3145\n",
      "Time taken for one epoch: 21.512677431106567 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she has not solve other and and and she can't solve the conversation [EOS]\n",
      "\n",
      "Epoch 46, Loss 1.2793\n",
      "Time taken for one epoch: 21.65097141265869 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just returned from the park but didn't find anything [EOS]\n",
      "\n",
      "Epoch 47, Loss 1.2468\n",
      "Time taken for one epoch: 21.645718097686768 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda doesn't know if she has any news that and is still depressed [EOS]\n",
      "\n",
      "Epoch 48, Loss 1.2092\n",
      "Time taken for one epoch: 21.439465045928955 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is upset because she didn't know anything as she can't find anything interesting for amanda [EOS]\n",
      "\n",
      "Epoch 49, Loss 1.1773\n",
      "Time taken for one epoch: 21.54583215713501 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and amanda have just taken to the cinema at jane's request [EOS]\n",
      "\n",
      "Epoch 50, Loss 1.1478\n",
      "Time taken for one epoch: 21.452200889587402 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just taken up the way from high [EOS]\n"
     ]
    }
   ],
   "source": [
    "# Take an example from the test set, to monitor it during training\n",
    "test_example = 0\n",
    "true_summary = summary_test[test_example]\n",
    "true_document = document_test[test_example]\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    number_of_batches=len(list(enumerate(dataset)))\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
    "        train_step(transformer, inp, tar)\n",
    "\n",
    "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
    "    losses.append(train_loss.result())\n",
    "\n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    print('Example summarization on the test set:')\n",
    "    print('  True summarization:')\n",
    "    print(f'    {true_summary}')\n",
    "    print('  Predicted summarization:')\n",
    "    print(f'    {summarize(transformer, true_document)}\\n')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edc84807021222b4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704637744822,
     "user_tz": -420,
     "elapsed": 1126407,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "987df75b-5610-4869-f531-00bbc7f1ced4"
   },
   "id": "edc84807021222b4"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "b40afb6b685a3e7f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704639299024,
     "user_tz": -420,
     "elapsed": 1422,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "972aedfb-d520-4a5d-9938-c75debc2688a"
   },
   "id": "b40afb6b685a3e7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarization"
   ],
   "metadata": {
    "collapsed": false,
    "id": "842e2e03b3255cac"
   },
   "id": "842e2e03b3255cac"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set example:\n",
      "[SOS] william: hey. today i saw you were arguing with blackett.  william: are you guys fine?  elizabeth: hi. sorry you had to see us argue.  elizabeth: it was just a small misunderstanding but we will solve it.  william: hope so  william: you think i should to talk to him about it?  elizabeth: no don't  elizabeth: he won't like it that we talked after the argument.  william: ok. but if you need any help, don't hesitate to call me  elizabeth: definitely [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] elizabeth had an argument with blackett today, but she doesn't want william to intermeddle. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] elizabeth had a problem with blackett today but she doesn't want william to intermeddle [EOS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_set_example = 42\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document[training_set_example]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "266f5df787ecf814",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704640283260,
     "user_tz": -420,
     "elapsed": 1655,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "c172397d-9812-43ff-ee60-eda47cdc3800"
   },
   "id": "266f5df787ecf814"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test set example:\n",
      "[SOS] will: hey babe, what do you want for dinner tonight?  emma:  gah, don't even worry about it tonight  will: what do you mean? everything ok?  emma: not really, but it's ok, don't worry about cooking though, i'm not hungry  will: well what time will you be home?  emma: soon, hopefully  will: you sure? maybe you want me to pick you up?  emma: no no it's alright. i'll be home soon, i'll tell you when i get home.   will: alright, love you.   emma: love you too.  [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] emma will be home soon and she will let will know. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] emma will pick emma up at dinner tonight and will be home around [EOS]\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "test_set_example = 3\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(document_test[test_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary_test[test_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document_test[test_set_example]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c568b2b187e74347",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704639502330,
     "user_tz": -420,
     "elapsed": 671,
     "user": {
      "displayName": "Thuan Nguyen",
      "userId": "08117843644571562719"
     }
    },
    "outputId": "086827d0-2c47-420b-c32c-521802d9f089"
   },
   "id": "c568b2b187e74347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
